{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyspark exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset and setup the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import os \n",
    "\n",
    "os.environ['JAVA_HOME']=\"/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home/\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master local[2] pyspark-shell\"\n",
    "\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../data/labelled_dataset.csv.gz\"\n",
    "raw_data = sc.textFile(data_file).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(data_file,header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label', 'txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'txt'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: string (nullable = true)\n",
      " |-- txt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(label='ham', txt='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...')\n",
      "\n",
      "\n",
      "Row(label='ham', txt='Ok lar... Joking wif u oni...')\n",
      "\n",
      "\n",
      "Row(label='spam', txt=\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\")\n",
      "\n",
      "\n",
      "Row(label='ham', txt='U dun say so early hor... U c already then say...')\n",
      "\n",
      "\n",
      "Row(label='ham', txt=\"Nah I don't think he goes to usf, he lives around here though\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Didn't strictly need a for loop, could have just then head()\n",
    "for row in df.head(5):\n",
    "    print(row)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|               label|               txt|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|             1970119|            840730|\n",
      "|   mean|            Infinity|1023.1263393359594|\n",
      "| stddev|                 NaN|  8450.10290912209|\n",
      "|    min|                   !|                  |\n",
      "|    max|⸪ Great Allowance...|        ”” said he|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\"label == 'spam'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(\"label == 'books'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\n",
    "import pyspark.sql.functions as f\n",
    "data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+\n",
      "|label|                 txt|wordCount|\n",
      "+-----+--------------------+---------+\n",
      "|  ham|Go until jurong p...|       20|\n",
      "|  ham|Ok lar... Joking ...|        6|\n",
      "| spam|Free entry in 2 a...|       28|\n",
      "|  ham|U dun say so earl...|       11|\n",
      "|  ham|Nah I don't think...|       13|\n",
      "| spam|FreeMsg Hey there...|       32|\n",
      "|  ham|Even my brother i...|       16|\n",
      "|  ham|As per your reque...|       26|\n",
      "| spam|WINNER!! As a val...|       26|\n",
      "| spam|Had your mobile 1...|       29|\n",
      "|  ham|I'm gonna be home...|       21|\n",
      "| spam|SIX chances to wi...|       26|\n",
      "| spam|URGENT! You have ...|       26|\n",
      "|  ham|I've been searchi...|       37|\n",
      "|  ham|I HAVE A DATE ON ...|        8|\n",
      "| spam|XXXMobileMovieClu...|       19|\n",
      "|  ham|Oh k...i'm watchi...|        4|\n",
      "|  ham|Eh u remember how...|       19|\n",
      "|  ham|Fine if thatåÕs t...|       13|\n",
      "| spam|England v Macedon...|       24|\n",
      "+-----+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word count for each row\n",
    "df = df.withColumn('wordCount', f.size(f.split(f.col('txt'), ' ')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(wordCount)=5584925)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total word\n",
    "df.select(f.sum('wordCount')).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what type df is\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='ham', txt='Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', wordCount=20),\n",
       " Row(label='ham', txt='Ok lar... Joking wif u oni...', wordCount=6),\n",
       " Row(label='spam', txt=\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", wordCount=28),\n",
       " Row(label='ham', txt='U dun say so early hor... U c already then say...', wordCount=11),\n",
       " Row(label='ham', txt=\"Nah I don't think he goes to usf, he lives around here though\", wordCount=13)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 row\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+\n",
      "|label|                 txt|wordCount|\n",
      "+-----+--------------------+---------+\n",
      "|  ham|Go until jurong p...|       20|\n",
      "|  ham|Ok lar... Joking ...|        6|\n",
      "| spam|Free entry in 2 a...|       28|\n",
      "|  ham|U dun say so earl...|       11|\n",
      "|  ham|Nah I don't think...|       13|\n",
      "| spam|FreeMsg Hey there...|       32|\n",
      "|  ham|Even my brother i...|       16|\n",
      "|  ham|As per your reque...|       26|\n",
      "| spam|WINNER!! As a val...|       26|\n",
      "| spam|Had your mobile 1...|       29|\n",
      "|  ham|I'm gonna be home...|       21|\n",
      "| spam|SIX chances to wi...|       26|\n",
      "| spam|URGENT! You have ...|       26|\n",
      "|  ham|I've been searchi...|       37|\n",
      "|  ham|I HAVE A DATE ON ...|        8|\n",
      "| spam|XXXMobileMovieClu...|       19|\n",
      "|  ham|Oh k...i'm watchi...|        4|\n",
      "|  ham|Eh u remember how...|       19|\n",
      "|  ham|Fine if thatåÕs t...|       13|\n",
      "| spam|England v Macedon...|       24|\n",
      "+-----+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word count collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "|    |784389|\n",
      "| the|282718|\n",
      "| and|194113|\n",
      "|  of|163358|\n",
      "|  to|145701|\n",
      "|   a|103066|\n",
      "|  in| 90602|\n",
      "|   I| 88815|\n",
      "|that| 72731|\n",
      "|  he| 53735|\n",
      "| his| 48351|\n",
      "|  it| 46497|\n",
      "|  as| 46428|\n",
      "|with| 45537|\n",
      "| was| 45149|\n",
      "|  is| 43232|\n",
      "| you| 42713|\n",
      "| for| 41989|\n",
      "|  my| 39440|\n",
      "|  be| 37498|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('word', f.explode(f.split(f.col('txt'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = df.withColumn('word', f.explode(f.split(f.col('txt'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_list = [list(row) for row in word.collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 784389],\n",
       " ['the', 282718],\n",
       " ['and', 194113],\n",
       " ['of', 163358],\n",
       " ['to', 145701],\n",
       " ['a', 103066],\n",
       " ['in', 90602],\n",
       " ['I', 88815],\n",
       " ['that', 72731],\n",
       " ['he', 53735],\n",
       " ['his', 48351],\n",
       " ['it', 46497],\n",
       " ['as', 46428],\n",
       " ['with', 45537],\n",
       " ['was', 45149],\n",
       " ['is', 43232],\n",
       " ['you', 42713],\n",
       " ['for', 41989],\n",
       " ['my', 39440],\n",
       " ['be', 37498],\n",
       " ['not', 36717],\n",
       " ['said', 35815],\n",
       " ['but', 30960],\n",
       " ['her', 29836],\n",
       " ['by', 28872],\n",
       " ['which', 27959],\n",
       " ['had', 27774],\n",
       " ['have', 27517],\n",
       " ['at', 26732],\n",
       " ['on', 24268],\n",
       " ['all', 23451],\n",
       " ['she', 23229],\n",
       " ['or', 23213],\n",
       " ['”', 22745],\n",
       " ['they', 22602],\n",
       " ['this', 22290],\n",
       " ['from', 22200],\n",
       " ['so', 21598],\n",
       " ['me', 20026],\n",
       " ['him', 19725],\n",
       " ['are', 18955],\n",
       " ['their', 16713],\n",
       " ['your', 16205],\n",
       " ['we', 16088],\n",
       " ['no', 16000],\n",
       " ['will', 15928],\n",
       " ['an', 15740],\n",
       " ['if', 15532],\n",
       " ['who', 15366],\n",
       " ['were', 14681],\n",
       " ['when', 14664],\n",
       " ['one', 14050],\n",
       " ['what', 13680],\n",
       " [\"'\", 12522],\n",
       " ['them', 12275],\n",
       " ['would', 12184],\n",
       " ['been', 11145],\n",
       " ['more', 11038],\n",
       " ['there', 10798],\n",
       " ['shall', 10633],\n",
       " ['our', 10372],\n",
       " ['The', 10160],\n",
       " ['upon', 10116],\n",
       " ['any', 9890],\n",
       " ['do', 9882],\n",
       " ['out', 9677],\n",
       " ['very', 9533],\n",
       " ['then', 9175],\n",
       " ['Mr.', 9140],\n",
       " ['some', 9013],\n",
       " ['may', 8949],\n",
       " ['into', 8922],\n",
       " ['und', 8877],\n",
       " ['up', 8810],\n",
       " ['than', 8440],\n",
       " ['man', 8333],\n",
       " ['now', 8325],\n",
       " ['has', 8145],\n",
       " ['like', 8114],\n",
       " ['can', 8037],\n",
       " ['—', 7915],\n",
       " ['should', 7783],\n",
       " ['such', 7762],\n",
       " ['know', 7676],\n",
       " ['Sir', 7654],\n",
       " ['could', 7602],\n",
       " ['must', 7425],\n",
       " ['He', 7316],\n",
       " ['am', 7223],\n",
       " ['see', 6903],\n",
       " ['good', 6854],\n",
       " ['great', 6842],\n",
       " ['other', 6763],\n",
       " ['much', 6739],\n",
       " ['But', 6733],\n",
       " ['little', 6711],\n",
       " ['only', 6611],\n",
       " ['did', 6542],\n",
       " ['made', 6490],\n",
       " ['And', 6460],\n",
       " ['where', 6287],\n",
       " ['about', 6287],\n",
       " ['before', 6227],\n",
       " ['time', 6177],\n",
       " ['thy', 6128],\n",
       " ['thou', 6020],\n",
       " ['’', 5947],\n",
       " ['come', 5911],\n",
       " ['die', 5888],\n",
       " ['never', 5830],\n",
       " ['say', 5808],\n",
       " ['make', 5799],\n",
       " ['how', 5782],\n",
       " ['after', 5739],\n",
       " ['yet', 5733],\n",
       " ['these', 5613],\n",
       " ['think', 5611],\n",
       " ['us', 5576],\n",
       " ['der', 5568],\n",
       " ['most', 5537],\n",
       " ['well', 5507],\n",
       " ['those', 5444],\n",
       " ['two', 5421],\n",
       " ['own', 5395],\n",
       " ['first', 5336],\n",
       " ['go', 5093],\n",
       " ['every', 5080],\n",
       " ['being', 5054],\n",
       " ['too', 4965],\n",
       " ['its', 4962],\n",
       " ['down', 4871],\n",
       " ['It', 4849],\n",
       " ['let', 4789],\n",
       " ['without', 4744],\n",
       " ['take', 4579],\n",
       " ['old', 4564],\n",
       " ['might', 4508],\n",
       " ['many', 4449],\n",
       " ['over', 4329],\n",
       " ['though', 4312],\n",
       " ['came', 4124],\n",
       " ['nor', 4068],\n",
       " ['it.', 4046],\n",
       " ['dear', 4035],\n",
       " ['same', 4012],\n",
       " ['here', 3980],\n",
       " ['men', 3936],\n",
       " ['thought', 3927],\n",
       " ['long', 3852],\n",
       " ['went', 3841],\n",
       " ['himself', 3800],\n",
       " ['give', 3721],\n",
       " ['Lord', 3714],\n",
       " ['even', 3677],\n",
       " ['She', 3648],\n",
       " [\"don't\", 3622],\n",
       " ['way', 3619],\n",
       " ['tell', 3618],\n",
       " ['day', 3540],\n",
       " ['ever', 3520],\n",
       " ['still', 3519],\n",
       " ['because', 3466],\n",
       " ['against', 3435],\n",
       " ['while', 3415],\n",
       " ['whom', 3379],\n",
       " ['just', 3371],\n",
       " ['last', 3332],\n",
       " ['Mr', 3307],\n",
       " ['God', 3301],\n",
       " ['thee', 3256],\n",
       " ['whose', 3246],\n",
       " ['young', 3236],\n",
       " ['nothing', 3206],\n",
       " ['put', 3198],\n",
       " ['Mrs.', 3182],\n",
       " ['ye', 3122],\n",
       " ['under', 3100],\n",
       " ['hand', 3070],\n",
       " ['found', 3057],\n",
       " ['him.', 3039],\n",
       " ['You', 2974],\n",
       " ['through', 2955],\n",
       " ['Miss', 2927],\n",
       " ['sir', 2910],\n",
       " ['both', 2900],\n",
       " ['till', 2888],\n",
       " ['away', 2860],\n",
       " ['Lady', 2850],\n",
       " ['hath', 2831],\n",
       " ['again', 2826],\n",
       " ['sie', 2818],\n",
       " ['\"\"', 2818],\n",
       " ['back', 2817],\n",
       " ['thing', 2806],\n",
       " ['life', 2795],\n",
       " ['off', 2783],\n",
       " ['another', 2757],\n",
       " ['find', 2754],\n",
       " ['love', 2746],\n",
       " ['get', 2741],\n",
       " ['saw', 2726],\n",
       " ['things', 2693],\n",
       " ['told', 2686],\n",
       " ['A', 2668],\n",
       " ['er', 2664],\n",
       " ['once', 2664],\n",
       " ['each', 2658],\n",
       " ['people', 2645],\n",
       " ['always', 2640],\n",
       " ['having', 2630],\n",
       " ['They', 2610],\n",
       " ['That', 2581],\n",
       " ['part', 2576],\n",
       " ['place', 2551],\n",
       " ['unto', 2541],\n",
       " ['eyes', 2536],\n",
       " ['took', 2513],\n",
       " ['left', 2507],\n",
       " ['den', 2501],\n",
       " ['cannot', 2494],\n",
       " ['me.', 2490],\n",
       " ['replied', 2486],\n",
       " ['heard', 2485],\n",
       " ['got', 2481],\n",
       " ['If', 2473],\n",
       " ['daß', 2468],\n",
       " ['heart', 2442],\n",
       " ['done', 2434],\n",
       " ['mind', 2407],\n",
       " ['also', 2403],\n",
       " ['das', 2397],\n",
       " ['says', 2393],\n",
       " ['indeed', 2391],\n",
       " ['three', 2389],\n",
       " ['better', 2357],\n",
       " ['right', 2350],\n",
       " ['therefore', 2340],\n",
       " ['far', 2326],\n",
       " ['look', 2323],\n",
       " ['since', 2314],\n",
       " ['however', 2298],\n",
       " [\"I'll\", 2297],\n",
       " ['call', 2284],\n",
       " ['between', 2279],\n",
       " ['new', 2270],\n",
       " ['knew', 2258],\n",
       " ['whole', 2258],\n",
       " ['house', 2238],\n",
       " ['name', 2228],\n",
       " ['want', 2219],\n",
       " ['called', 2202],\n",
       " ['ich', 2193],\n",
       " ['set', 2191],\n",
       " ['zu', 2185],\n",
       " ['best', 2178],\n",
       " ['among', 2160],\n",
       " ['going', 2133],\n",
       " ['believe', 2130],\n",
       " ['head', 2128],\n",
       " ['sure', 2124],\n",
       " ['face', 2118],\n",
       " ['This', 2107],\n",
       " ['you.', 2103],\n",
       " ['few', 2092],\n",
       " [\"I'm\", 2091],\n",
       " ['world', 2060],\n",
       " ['rather', 2046],\n",
       " ['soon', 2042],\n",
       " ['father', 2024],\n",
       " ['why', 2021],\n",
       " ['In', 2012],\n",
       " ['hope', 2002],\n",
       " ['We', 1976],\n",
       " ['nicht', 1960],\n",
       " ['poor', 1951],\n",
       " ['seen', 1929],\n",
       " ['perhaps', 1926],\n",
       " ['brought', 1917],\n",
       " ['O', 1912],\n",
       " ['What', 1909],\n",
       " ['said.', 1904],\n",
       " ['friend', 1903],\n",
       " ['&', 1898],\n",
       " ['thus', 1894],\n",
       " ['word', 1892],\n",
       " ['For', 1888],\n",
       " ['present', 1885],\n",
       " ['something', 1884],\n",
       " ['dem', 1883],\n",
       " ['hear', 1869],\n",
       " ['night', 1868],\n",
       " ['looked', 1854],\n",
       " ['gave', 1853],\n",
       " ['full', 1849],\n",
       " ['half', 1839],\n",
       " [\"'tis\", 1838],\n",
       " ['enough', 1830],\n",
       " ['taken', 1811],\n",
       " ['There', 1808],\n",
       " ['von', 1798],\n",
       " ['almost', 1781],\n",
       " ['least', 1777],\n",
       " ['less', 1768],\n",
       " ['cried', 1764],\n",
       " ['“I', 1755],\n",
       " ['Mrs', 1754],\n",
       " ['leave', 1753],\n",
       " ['whether', 1749],\n",
       " ['next', 1746],\n",
       " ['round', 1744],\n",
       " [\"tho'\", 1728],\n",
       " ['course', 1724],\n",
       " ['woman', 1723],\n",
       " ['them.', 1717],\n",
       " ['quite', 1706],\n",
       " ['does', 1702],\n",
       " ['true', 1693],\n",
       " ['haue', 1684],\n",
       " ['began', 1682],\n",
       " ['work', 1677],\n",
       " ['given', 1676],\n",
       " ['themselves', 1665],\n",
       " ['King', 1659],\n",
       " ['either', 1651],\n",
       " ['kind', 1635],\n",
       " ['myself', 1633],\n",
       " ['seemed', 1627],\n",
       " ['To', 1625],\n",
       " ['speak', 1623],\n",
       " ['des', 1623],\n",
       " ['ist', 1607],\n",
       " ['years', 1604],\n",
       " ['sent', 1600],\n",
       " ['words', 1597],\n",
       " ['wish', 1584],\n",
       " ['her.', 1579],\n",
       " ['asked', 1568],\n",
       " ['keep', 1562],\n",
       " ['Then', 1559],\n",
       " ['hands', 1552],\n",
       " ['near', 1542],\n",
       " ['light', 1540],\n",
       " ['mean', 1532],\n",
       " ['side', 1529],\n",
       " ['end', 1522],\n",
       " ['answered', 1520],\n",
       " ['above', 1515],\n",
       " ['returned', 1502],\n",
       " ['ought', 1500],\n",
       " ['As', 1498],\n",
       " ['saying', 1497],\n",
       " ['&c.', 1475],\n",
       " ['son', 1464],\n",
       " ['voice', 1453],\n",
       " ['others', 1446],\n",
       " ['order', 1441],\n",
       " ['du', 1427],\n",
       " ['reason', 1424],\n",
       " ['several', 1421],\n",
       " ['bring', 1420],\n",
       " ['mother', 1420],\n",
       " ['When', 1420],\n",
       " ['power', 1419],\n",
       " ['suppose', 1410],\n",
       " ['moment', 1408],\n",
       " ['small', 1408],\n",
       " ['together', 1407],\n",
       " ['often', 1405],\n",
       " ['mit', 1396],\n",
       " ['lay', 1385],\n",
       " ['looking', 1379],\n",
       " ['home', 1371],\n",
       " ['stood', 1368],\n",
       " ['neither', 1345],\n",
       " ['His', 1344],\n",
       " ['Man', 1343],\n",
       " ['herself', 1343],\n",
       " ['felt', 1338],\n",
       " ['high', 1337],\n",
       " ['matter', 1337],\n",
       " ['LORD', 1332],\n",
       " ['use', 1314],\n",
       " ['within', 1312],\n",
       " ['days', 1312],\n",
       " ['war', 1308],\n",
       " ['My', 1305],\n",
       " ['gone', 1302],\n",
       " ['care', 1301],\n",
       " ['certain', 1282],\n",
       " ['auf', 1282],\n",
       " ['ihr', 1279],\n",
       " [\"it's\", 1276],\n",
       " ['fear', 1275],\n",
       " [\"can't\", 1258],\n",
       " ['manner', 1258],\n",
       " ['rest', 1258],\n",
       " ['turned', 1251],\n",
       " ['free', 1243],\n",
       " ['read', 1235],\n",
       " ['art', 1230],\n",
       " ['live', 1228],\n",
       " ['seems', 1226],\n",
       " ['none', 1219],\n",
       " ['So', 1209],\n",
       " ['room', 1209],\n",
       " ['ask', 1209],\n",
       " ['known', 1208],\n",
       " ['alone', 1207],\n",
       " ['mine', 1203],\n",
       " ['Madam', 1198],\n",
       " ['feel', 1187],\n",
       " ['hundred', 1185],\n",
       " ['country', 1184],\n",
       " ['lady', 1184],\n",
       " ['money', 1183],\n",
       " ['open', 1181],\n",
       " ['es', 1176],\n",
       " ['nature', 1173],\n",
       " ['four', 1172],\n",
       " [\"that's\", 1171],\n",
       " ['different', 1165],\n",
       " ['morning', 1162],\n",
       " ['thousand', 1160],\n",
       " ['lost', 1156],\n",
       " ['general', 1156],\n",
       " ['hold', 1148],\n",
       " ['doubt', 1147],\n",
       " ['send', 1145],\n",
       " ['means', 1144],\n",
       " ['ein', 1142],\n",
       " ['short', 1136],\n",
       " ['sat', 1135],\n",
       " ['hast', 1131],\n",
       " ['death', 1131],\n",
       " [\"''\", 1131],\n",
       " ['times', 1129],\n",
       " ['late', 1129],\n",
       " ['door', 1126],\n",
       " ['wife', 1124],\n",
       " ['Tom', 1119],\n",
       " ['please', 1118],\n",
       " ['strong', 1114],\n",
       " ['wie', 1113],\n",
       " ['John', 1112],\n",
       " ['along', 1111],\n",
       " ['makes', 1111],\n",
       " ['large', 1111],\n",
       " ['brother', 1109],\n",
       " ['body', 1103],\n",
       " ['continued', 1102],\n",
       " ['common', 1093],\n",
       " ['really', 1092],\n",
       " ['stand', 1091],\n",
       " ['de', 1090],\n",
       " ['already', 1087],\n",
       " ['according', 1085],\n",
       " ['hat', 1085],\n",
       " ['towards', 1081],\n",
       " ['happy', 1081],\n",
       " ['answer', 1074],\n",
       " ['help', 1072],\n",
       " ['hour', 1070],\n",
       " ['making', 1069],\n",
       " ['forth', 1067],\n",
       " ['How', 1062],\n",
       " ['cause', 1060],\n",
       " ['truth', 1057],\n",
       " ['coming', 1056],\n",
       " ['land', 1056],\n",
       " ['it;', 1051],\n",
       " ['used', 1050],\n",
       " ['se', 1050],\n",
       " ['need', 1048],\n",
       " ['fine', 1045],\n",
       " ['comes', 1037],\n",
       " ['madam', 1034],\n",
       " ['anything', 1033],\n",
       " ['girl', 1025],\n",
       " ['talk', 1023],\n",
       " ['received', 1020],\n",
       " ['kept', 1020],\n",
       " ['i', 1019],\n",
       " ['fair', 1019],\n",
       " ['letter', 1016],\n",
       " ['state', 1013],\n",
       " ['fell', 1010],\n",
       " ['able', 1009],\n",
       " ['sense', 1007],\n",
       " ['friends', 1005],\n",
       " ['dead', 1004],\n",
       " ['an’', 997],\n",
       " ['children', 995],\n",
       " ['meet', 991],\n",
       " ['business', 991],\n",
       " ['turn', 990],\n",
       " ['hard', 990],\n",
       " ['Captain', 988],\n",
       " ['No', 986],\n",
       " ['pretty', 983],\n",
       " ['Sir.', 982],\n",
       " ['soul', 979],\n",
       " ['THE', 979],\n",
       " ['person', 978],\n",
       " ['return', 978],\n",
       " ['else', 968],\n",
       " ['child', 968],\n",
       " ['.', 965],\n",
       " ['certainly', 962],\n",
       " ['account', 959],\n",
       " ['honour', 958],\n",
       " ['p.', 952],\n",
       " ['seem', 951],\n",
       " ['passed', 949],\n",
       " ['white', 948],\n",
       " ['Men', 947],\n",
       " ['necessary', 946],\n",
       " ['subject', 945],\n",
       " ['air', 945],\n",
       " ['eye', 940],\n",
       " ['sort', 934],\n",
       " ['case', 934],\n",
       " ['fire', 934],\n",
       " ['human', 929],\n",
       " ['run', 929],\n",
       " [\"I've\", 925],\n",
       " ['gentleman', 923],\n",
       " ['taking', 921],\n",
       " ['behind', 921],\n",
       " ['sometimes', 920],\n",
       " ['king', 919],\n",
       " ['remember', 916],\n",
       " ['sich', 915],\n",
       " ['bear', 910],\n",
       " ['become', 907],\n",
       " ['second', 907],\n",
       " ['bad', 905],\n",
       " ['Now', 903],\n",
       " ['laid', 902],\n",
       " ['five', 902],\n",
       " ['past', 896],\n",
       " ['sweet', 893],\n",
       " ['At', 891],\n",
       " ['ten', 890],\n",
       " ['particular', 888],\n",
       " ['pay', 885],\n",
       " ['Gott', 881],\n",
       " ['desire', 879],\n",
       " ['wird', 878],\n",
       " ['2', 873],\n",
       " ['water', 870],\n",
       " ['year', 868],\n",
       " ['met', 867],\n",
       " ['pray', 866],\n",
       " ['ready', 861],\n",
       " ['close', 857],\n",
       " [\"won't\", 857],\n",
       " ['receive', 857],\n",
       " ['held', 857],\n",
       " ['quod', 856],\n",
       " ['-', 856],\n",
       " ['act', 854],\n",
       " ['Father', 852],\n",
       " ['point', 845],\n",
       " ['natural', 844],\n",
       " ['HERR', 843],\n",
       " ['form', 834],\n",
       " ['beyond', 834],\n",
       " ['Why', 833],\n",
       " ['fall', 833],\n",
       " ['cut', 828],\n",
       " ['stay', 827],\n",
       " ['proper', 826],\n",
       " ['women', 826],\n",
       " ['denn', 825],\n",
       " ['earth', 824],\n",
       " ['observed', 823],\n",
       " ['u', 823],\n",
       " ['during', 822],\n",
       " ['family', 816],\n",
       " ['Israel', 816],\n",
       " ['glad', 814],\n",
       " ['pleasure', 813],\n",
       " ['again.', 810],\n",
       " ['sight', 810],\n",
       " ['English', 809],\n",
       " ['feet', 806],\n",
       " ['number', 806],\n",
       " ['greater', 803],\n",
       " ['following', 802],\n",
       " ['et', 800],\n",
       " ['immediately', 799],\n",
       " ['write', 799],\n",
       " ['save', 797],\n",
       " ['lord', 794],\n",
       " ['itself', 793],\n",
       " ['self', 792],\n",
       " ['understand', 792],\n",
       " ['knows', 790],\n",
       " ['sound', 790],\n",
       " ['spirit', 789],\n",
       " ['strange', 788],\n",
       " ['ne', 787],\n",
       " [\"th'\", 787],\n",
       " ['euch', 787],\n",
       " ['All', 784],\n",
       " ['town', 779],\n",
       " ['St.', 773],\n",
       " ['question', 771],\n",
       " ['boy', 768],\n",
       " ['until', 765],\n",
       " ['deep', 761],\n",
       " ['afraid', 760],\n",
       " ['ihm', 758],\n",
       " ['ihn', 757],\n",
       " ['shew', 757],\n",
       " ['spoke', 757],\n",
       " ['change', 755],\n",
       " ['low', 754],\n",
       " ['public', 754],\n",
       " ['man.', 752],\n",
       " ['Let', 752],\n",
       " ['him;', 749],\n",
       " ['als', 747],\n",
       " [\"he's\", 746],\n",
       " ['aber', 746],\n",
       " ['former', 745],\n",
       " ['possible', 745],\n",
       " ['follow', 744],\n",
       " ['appear', 741],\n",
       " ['further', 741],\n",
       " ['opinion', 741],\n",
       " ['city', 740],\n",
       " ['consider', 740],\n",
       " ['all.', 739],\n",
       " ['HERRN', 739],\n",
       " ['ground', 738],\n",
       " ['sein', 735],\n",
       " ['yes', 735],\n",
       " ['Anne', 734],\n",
       " [\"didn't\", 734],\n",
       " ['added', 732],\n",
       " ['shalt', 730],\n",
       " ['doth', 728],\n",
       " [\"wou'd\", 727],\n",
       " ['pass', 727],\n",
       " ['House', 725],\n",
       " ['cold', 724],\n",
       " ['except', 722],\n",
       " ['arms', 722],\n",
       " ['black', 721],\n",
       " ['alle', 720],\n",
       " ['show', 719],\n",
       " ['behold', 718],\n",
       " ['Jesus', 718],\n",
       " ['vor', 717],\n",
       " ['view', 716],\n",
       " ['daughter', 715],\n",
       " ['sister', 715],\n",
       " ['Love', 714],\n",
       " ['Prince', 713],\n",
       " ['dare', 711],\n",
       " ['longer', 707],\n",
       " ['seeing', 707],\n",
       " ['greatest', 706],\n",
       " ['clear', 705],\n",
       " ['sit', 702],\n",
       " ['ill', 698],\n",
       " [\"there's\", 697],\n",
       " ['Master', 690],\n",
       " ['interest', 689],\n",
       " ['led', 689],\n",
       " ['company', 688],\n",
       " ['try', 686],\n",
       " ['wanted', 686],\n",
       " ['dark', 685],\n",
       " ['master', 684],\n",
       " ['One', 681],\n",
       " ['carried', 678],\n",
       " ['force', 678],\n",
       " ['Her', 677],\n",
       " ['People', 676],\n",
       " ['peace', 674],\n",
       " ['Sohn', 673],\n",
       " ['wise', 673],\n",
       " ['early', 673],\n",
       " ['Nature', 672],\n",
       " ['Of', 672],\n",
       " ['idea', 672],\n",
       " ['deal', 671],\n",
       " ['honest', 669],\n",
       " ['written', 667],\n",
       " ['blood', 666],\n",
       " ['real', 665],\n",
       " ['Do', 665],\n",
       " ['seine', 664],\n",
       " ['fellow', 664],\n",
       " ['fit', 663],\n",
       " ['Your', 662],\n",
       " ['evening', 660],\n",
       " ['occasion', 658],\n",
       " ['whatever', 658],\n",
       " ['me;', 658],\n",
       " ['character', 657],\n",
       " ['law', 657],\n",
       " ['cast', 656],\n",
       " ['Time', 655],\n",
       " ['doing', 654],\n",
       " ['hardly', 654],\n",
       " ['im', 653],\n",
       " ['easy', 653],\n",
       " ['fact', 652],\n",
       " ['Is', 652],\n",
       " ['followed', 651],\n",
       " ['May', 645],\n",
       " ['regard', 644],\n",
       " [\"you'll\", 643],\n",
       " ['became', 641],\n",
       " ['six', 638],\n",
       " ['age', 638],\n",
       " ['you?', 638],\n",
       " ['noble', 638],\n",
       " ['aus', 635],\n",
       " ['OF', 634],\n",
       " ['thank', 634],\n",
       " ['he.', 633],\n",
       " ['purpose', 632],\n",
       " ['s', 632],\n",
       " ['French', 631],\n",
       " ['effect', 631],\n",
       " ['sea', 629],\n",
       " ['appeared', 625],\n",
       " ['bed', 625],\n",
       " ['thoughts', 625],\n",
       " ['By', 624],\n",
       " ['England', 623],\n",
       " ['carry', 621],\n",
       " ['across', 620],\n",
       " ['yourself', 617],\n",
       " ['plain', 617],\n",
       " ['I.', 617],\n",
       " ['worth', 616],\n",
       " ['nach', 616],\n",
       " ['thinking', 616],\n",
       " ['uncle', 615],\n",
       " ['object', 614],\n",
       " ['expect', 614],\n",
       " ['World', 613],\n",
       " ['now.', 612],\n",
       " ['wonder', 612],\n",
       " ['knowledge', 612],\n",
       " ['Charles', 611],\n",
       " ['trust', 610],\n",
       " ['thine', 610],\n",
       " ['length', 609],\n",
       " ['unless', 609],\n",
       " ['um', 609],\n",
       " ['Und', 605],\n",
       " ['twenty', 605],\n",
       " ['husband', 605],\n",
       " ['respect', 604],\n",
       " ['Life', 603],\n",
       " ['rich', 602],\n",
       " ['ha', 600],\n",
       " ['equal', 599],\n",
       " ['mir', 599],\n",
       " ['bright', 599],\n",
       " ['beautiful', 598],\n",
       " ['vain', 597],\n",
       " ['walk', 596],\n",
       " ['parts', 592],\n",
       " ['single', 592],\n",
       " ['generally', 592],\n",
       " ['line', 592],\n",
       " ['gives', 590],\n",
       " ['strength', 589],\n",
       " ['über', 588],\n",
       " ['wait', 588],\n",
       " ['Oh', 588],\n",
       " [\"you're\", 587],\n",
       " ['al', 586],\n",
       " ['big', 586],\n",
       " ['especially', 582],\n",
       " ['gentle', 581],\n",
       " ['them;', 580],\n",
       " ['not.', 579],\n",
       " ['ere', 578],\n",
       " ['born', 578],\n",
       " ['David', 576],\n",
       " ['London', 575],\n",
       " ['ran', 575],\n",
       " ['learned', 574],\n",
       " ['König', 574],\n",
       " ['around', 574],\n",
       " ['so.', 574],\n",
       " ['giving', 573],\n",
       " ['book', 573],\n",
       " ['visit', 572],\n",
       " ['wrong', 572],\n",
       " ['worthy', 572],\n",
       " ['dir', 570],\n",
       " ['usual', 570],\n",
       " ['wild', 570],\n",
       " ['living', 570],\n",
       " ['time.', 569],\n",
       " ['play', 568],\n",
       " ['arm', 567],\n",
       " ['feeling', 567],\n",
       " ['easily', 567],\n",
       " ['Church', 566],\n",
       " ['trouble', 566],\n",
       " ['more.', 564],\n",
       " ['joy', 563],\n",
       " ['fortune', 563],\n",
       " ['rise', 562],\n",
       " ['grace', 561],\n",
       " ['us.', 561],\n",
       " ['private', 561],\n",
       " ['soft', 560],\n",
       " ['rose', 560],\n",
       " ['secret', 560],\n",
       " ['speaking', 559],\n",
       " ['persons', 559],\n",
       " ['begin', 559],\n",
       " ['draw', 556],\n",
       " ['toward', 555],\n",
       " ['forward', 555],\n",
       " ['entered', 554],\n",
       " ['On', 553],\n",
       " ['that.', 553],\n",
       " ['walked', 553],\n",
       " ['pride', 552],\n",
       " ['meant', 552],\n",
       " ['everything', 551],\n",
       " ['`I', 551],\n",
       " ['instead', 548],\n",
       " [\"'em\", 548],\n",
       " ['sind', 548],\n",
       " ['tears', 547],\n",
       " ['break', 546],\n",
       " ['false', 543],\n",
       " ['werden', 543],\n",
       " ['service', 541],\n",
       " ['bound', 541],\n",
       " ['turning', 540],\n",
       " ['mouth', 538],\n",
       " ['e', 538],\n",
       " ['Hand', 538],\n",
       " ['week', 538],\n",
       " ['mich', 538],\n",
       " ['address', 538],\n",
       " ['mighty', 537],\n",
       " ['auch', 536],\n",
       " ['Son', 536],\n",
       " ['passion', 536],\n",
       " ['sitting', 535],\n",
       " ['eat', 535],\n",
       " ['beginning', 535],\n",
       " ['concerning', 535],\n",
       " ['sake', 533],\n",
       " ['pleased', 531],\n",
       " ['day.', 530],\n",
       " ['Dr.', 529],\n",
       " ['worse', 529],\n",
       " ['sun', 528],\n",
       " ['looks', 527],\n",
       " ['opened', 527],\n",
       " ['road', 526],\n",
       " ['tender', 526],\n",
       " ['že', 525],\n",
       " ['gentlemen', 525],\n",
       " ['da', 525],\n",
       " ['soll', 524],\n",
       " ['ancient', 522],\n",
       " ['paid', 521],\n",
       " ['sleep', 521],\n",
       " ['drew', 520],\n",
       " ['contrary', 520],\n",
       " ['sir.', 519],\n",
       " ['married', 519],\n",
       " ['With', 519],\n",
       " ['dich', 519],\n",
       " ['evil', 518],\n",
       " [\"I'd\", 518],\n",
       " ['chief', 518],\n",
       " ['sudden', 518],\n",
       " ['away.', 518],\n",
       " ['sufficient', 517],\n",
       " ['Christ', 516],\n",
       " ['marry', 516],\n",
       " ['faith', 515],\n",
       " ['Lucy', 515],\n",
       " ['perfect', 514],\n",
       " ['youth', 514],\n",
       " ['repeated', 513],\n",
       " ['said:', 513],\n",
       " ['prove', 513],\n",
       " ['Duke', 512],\n",
       " ['latter', 512],\n",
       " ['proud', 512],\n",
       " ['sorry', 511],\n",
       " ['besides', 511],\n",
       " ['suddenly', 511],\n",
       " ['la', 511],\n",
       " ['it?', 510],\n",
       " [')', 509],\n",
       " ['obliged', 509],\n",
       " ['Ile', 508],\n",
       " ['“You', 508],\n",
       " ['figure', 507],\n",
       " ['charge', 507],\n",
       " ['na', 507],\n",
       " ['considered', 506],\n",
       " ['hair', 505],\n",
       " ['places', 505],\n",
       " ['way.', 504],\n",
       " ['nearly', 503],\n",
       " ['Friend', 503],\n",
       " ['red', 502],\n",
       " ['drawn', 502],\n",
       " ['particularly', 501],\n",
       " ['standing', 501],\n",
       " ['getting', 501],\n",
       " ['forget', 500],\n",
       " ['table', 500],\n",
       " ['future', 500],\n",
       " ['goes', 500],\n",
       " ['wir', 499],\n",
       " ['Power', 499],\n",
       " ['seven', 499],\n",
       " [\"thro'\", 499],\n",
       " ['Not', 499],\n",
       " ['there.', 498],\n",
       " ['impossible', 497],\n",
       " ['him:', 497],\n",
       " ['hate', 495],\n",
       " ['hours', 495],\n",
       " ['4', 494],\n",
       " ['afterwards', 493],\n",
       " ['offer', 493],\n",
       " ['sad', 492],\n",
       " ['für', 492],\n",
       " ['Queen', 491],\n",
       " ['chance', 490],\n",
       " ['out.', 490],\n",
       " ['below', 489],\n",
       " ['Mary', 487],\n",
       " ['serve', 485],\n",
       " ['ship', 484],\n",
       " ['story', 484],\n",
       " ['wind', 482],\n",
       " ['tried', 481],\n",
       " ['observe', 481],\n",
       " ['non', 480],\n",
       " ['struck', 479],\n",
       " ['lose', 479],\n",
       " ['knowing', 479],\n",
       " ['lie', 478],\n",
       " ['Thou', 478],\n",
       " ['beauty', 477],\n",
       " ['foot', 477],\n",
       " ['expected', 477],\n",
       " ['fast', 476],\n",
       " ['Will', 476],\n",
       " ['George', 476],\n",
       " ['learn', 476],\n",
       " ['Honour', 476],\n",
       " ['on.', 475],\n",
       " ['wenn', 474],\n",
       " ['other.', 474],\n",
       " ['thee.', 473],\n",
       " ['angry', 473],\n",
       " ['loved', 472],\n",
       " ['third', 472],\n",
       " ['heaven', 471],\n",
       " ['happened', 469],\n",
       " ['various', 469],\n",
       " ['whilst', 468],\n",
       " ['These', 468],\n",
       " ['probably', 468],\n",
       " ['appears', 467],\n",
       " [\"It's\", 467],\n",
       " ['saith', 466],\n",
       " ['servant', 465],\n",
       " ['lived', 465],\n",
       " ['mein', 465],\n",
       " ['lest', 464],\n",
       " ['allow', 463],\n",
       " [\"o'er\", 463],\n",
       " ['later', 463],\n",
       " ['degree', 463],\n",
       " ['Come', 462],\n",
       " ['supposed', 462],\n",
       " ['wide', 461],\n",
       " ['life.', 461],\n",
       " ['virtue', 460],\n",
       " ['wrote', 459],\n",
       " ['broken', 459],\n",
       " ['presently', 458],\n",
       " ['report', 458],\n",
       " ['Parliament', 458],\n",
       " ...]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-415755c1419f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "311334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+\n",
      "|label|                 txt|wordCount|               words|\n",
      "+-----+--------------------+---------+--------------------+\n",
      "|  ham|Go until jurong p...|       20|[go, until, juron...|\n",
      "|  ham|Ok lar... Joking ...|        6|[ok, lar, joking,...|\n",
      "| spam|Free entry in 2 a...|       28|[free, entry, in,...|\n",
      "|  ham|U dun say so earl...|       11|[u, dun, say, so,...|\n",
      "|  ham|Nah I don't think...|       13|[nah, i, don, t, ...|\n",
      "| spam|FreeMsg Hey there...|       32|[freemsg, hey, th...|\n",
      "|  ham|Even my brother i...|       16|[even, my, brothe...|\n",
      "|  ham|As per your reque...|       26|[as, per, your, r...|\n",
      "| spam|WINNER!! As a val...|       26|[winner, as, a, v...|\n",
      "| spam|Had your mobile 1...|       29|[had, your, mobil...|\n",
      "|  ham|I'm gonna be home...|       21|[i, m, gonna, be,...|\n",
      "| spam|SIX chances to wi...|       26|[six, chances, to...|\n",
      "| spam|URGENT! You have ...|       26|[urgent, you, hav...|\n",
      "|  ham|I've been searchi...|       37|[i, ve, been, sea...|\n",
      "|  ham|I HAVE A DATE ON ...|        8|[i, have, a, date...|\n",
      "| spam|XXXMobileMovieClu...|       19|[xxxmobilemoviecl...|\n",
      "|  ham|Oh k...i'm watchi...|        4|[oh, k, i, m, wat...|\n",
      "|  ham|Eh u remember how...|       19|[eh, u, remember,...|\n",
      "|  ham|Fine if thatåÕs t...|       13|[fine, if, that, ...|\n",
      "| spam|England v Macedon...|       24|[england, v, mace...|\n",
      "+-----+--------------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"txt\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"txt\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(df)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(df)\n",
    "\n",
    "regexTokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "|label|                 txt|wordCount|               words|            filtered|\n",
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|       20|[go, until, juron...|[go, jurong, poin...|\n",
      "|  ham|Ok lar... Joking ...|        6|[ok, lar, joking,...|[ok, lar, joking,...|\n",
      "| spam|Free entry in 2 a...|       28|[free, entry, in,...|[free, entry, 2, ...|\n",
      "|  ham|U dun say so earl...|       11|[u, dun, say, so,...|[u, dun, say, ear...|\n",
      "|  ham|Nah I don't think...|       13|[nah, i, don, t, ...|[nah, think, goes...|\n",
      "| spam|FreeMsg Hey there...|       32|[freemsg, hey, th...|[freemsg, hey, da...|\n",
      "|  ham|Even my brother i...|       16|[even, my, brothe...|[even, brother, l...|\n",
      "|  ham|As per your reque...|       26|[as, per, your, r...|[per, request, me...|\n",
      "| spam|WINNER!! As a val...|       26|[winner, as, a, v...|[winner, valued, ...|\n",
      "| spam|Had your mobile 1...|       29|[had, your, mobil...|[mobile, 11, mont...|\n",
      "|  ham|I'm gonna be home...|       21|[i, m, gonna, be,...|[m, gonna, home, ...|\n",
      "| spam|SIX chances to wi...|       26|[six, chances, to...|[six, chances, wi...|\n",
      "| spam|URGENT! You have ...|       26|[urgent, you, hav...|[urgent, won, 1, ...|\n",
      "|  ham|I've been searchi...|       37|[i, ve, been, sea...|[ve, searching, r...|\n",
      "|  ham|I HAVE A DATE ON ...|        8|[i, have, a, date...|      [date, sunday]|\n",
      "| spam|XXXMobileMovieClu...|       19|[xxxmobilemoviecl...|[xxxmobilemoviecl...|\n",
      "|  ham|Oh k...i'm watchi...|        4|[oh, k, i, m, wat...|[oh, k, m, watching]|\n",
      "|  ham|Eh u remember how...|       19|[eh, u, remember,...|[eh, u, remember,...|\n",
      "|  ham|Fine if thatåÕs t...|       13|[fine, if, that, ...|[fine, way, u, fe...|\n",
      "| spam|England v Macedon...|       24|[england, v, mace...|[england, v, mace...|\n",
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "data = remover.transform(regexTokenized).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = remover.transform(regexTokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "|label|                 txt|wordCount|               words|            filtered|\n",
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "|  ham|Go until jurong p...|       20|[go, until, juron...|[go, jurong, poin...|\n",
      "|  ham|Ok lar... Joking ...|        6|[ok, lar, joking,...|[ok, lar, joking,...|\n",
      "| spam|Free entry in 2 a...|       28|[free, entry, in,...|[free, entry, 2, ...|\n",
      "|  ham|U dun say so earl...|       11|[u, dun, say, so,...|[u, dun, say, ear...|\n",
      "|  ham|Nah I don't think...|       13|[nah, i, don, t, ...|[nah, think, goes...|\n",
      "| spam|FreeMsg Hey there...|       32|[freemsg, hey, th...|[freemsg, hey, da...|\n",
      "|  ham|Even my brother i...|       16|[even, my, brothe...|[even, brother, l...|\n",
      "|  ham|As per your reque...|       26|[as, per, your, r...|[per, request, me...|\n",
      "| spam|WINNER!! As a val...|       26|[winner, as, a, v...|[winner, valued, ...|\n",
      "| spam|Had your mobile 1...|       29|[had, your, mobil...|[mobile, 11, mont...|\n",
      "|  ham|I'm gonna be home...|       21|[i, m, gonna, be,...|[m, gonna, home, ...|\n",
      "| spam|SIX chances to wi...|       26|[six, chances, to...|[six, chances, wi...|\n",
      "| spam|URGENT! You have ...|       26|[urgent, you, hav...|[urgent, won, 1, ...|\n",
      "|  ham|I've been searchi...|       37|[i, ve, been, sea...|[ve, searching, r...|\n",
      "|  ham|I HAVE A DATE ON ...|        8|[i, have, a, date...|      [date, sunday]|\n",
      "| spam|XXXMobileMovieClu...|       19|[xxxmobilemoviecl...|[xxxmobilemoviecl...|\n",
      "|  ham|Oh k...i'm watchi...|        4|[oh, k, i, m, wat...|[oh, k, m, watching]|\n",
      "|  ham|Eh u remember how...|       19|[eh, u, remember,...|[eh, u, remember,...|\n",
      "|  ham|Fine if thatåÕs t...|       13|[fine, if, that, ...|[fine, way, u, fe...|\n",
      "| spam|England v Macedon...|       24|[england, v, mace...|[england, v, mace...|\n",
      "+-----+--------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o484.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 1078, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f9b500e2a828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mto_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \"\"\"\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o484.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 1078, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3255)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3255)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o485.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 1074, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-64af5de3c1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m311334\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o485.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 1074, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.ml.feature.CountVectorizer.fit(CountVectorizer.scala:230)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$createTransformFunc$2: (string) => array<string>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:143)\n\tat org.apache.spark.ml.feature.RegexTokenizer$$anonfun$createTransformFunc$2.apply(Tokenizer.scala:141)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=311334, minDF=2.0)\n",
    "\n",
    "model = cv.fit(data)\n",
    "\n",
    "result = model.transform(data)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-6fdbd4a2ea9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'list'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'txt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df.withColumn('list', df['txt'].split(\" \")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = f.split(df['txt'], ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('NAME1', split_col.getItem(0))\n",
    "data = data.withColumn('NAME2', split_col.getItem(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-db8d0b42c964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|word| count|\n",
      "+----+------+\n",
      "|    |784389|\n",
      "| the|282718|\n",
      "| and|194113|\n",
      "|  of|163358|\n",
      "|  to|145701|\n",
      "|   a|103066|\n",
      "|  in| 90602|\n",
      "|   I| 88815|\n",
      "|that| 72731|\n",
      "|  he| 53735|\n",
      "| his| 48351|\n",
      "|  it| 46497|\n",
      "|  as| 46428|\n",
      "|with| 45537|\n",
      "| was| 45149|\n",
      "|  is| 43232|\n",
      "| you| 42713|\n",
      "| for| 41989|\n",
      "|  my| 39440|\n",
      "|  be| 37498|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('word', f.explode(f.split(f.col('txt'), ' ')))\\\n",
    "    .groupBy('word')\\\n",
    "    .count()\\\n",
    "    .sort('count', ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"../books/*.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6003218"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|sentence                           |words                                     |tokens|\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "|Hi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\n",
      "|I wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\n",
      "|Logistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n",
      "+-----------------------------------+------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.select(\"sentence\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "regexTokenized.select(\"sentence\", \"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)\n",
    "\n",
    "print((df.count(), len(df.columns)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
